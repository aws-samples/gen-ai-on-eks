{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d7fa321",
   "metadata": {},
   "source": [
    "# GPT-J-6B Fine-Tuning with Ray Train and DeepSpeed on Amazon EKS\n",
    "\n",
    "This example showcases how to use Ray Train for GPT-J fine-tuning. GPT-J is a GPT-2-like causal language model trained on the Pile dataset. This particular model has 6 billion parameters. For more information, see GPT-J.\n",
    "\n",
    "This example uses the Ray Train ðŸ¤— Transformers integration and a pre-trained model from the Hugging Face Hub. Note that this example is adaptable to other similar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5963640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Installing core components\n",
    "! pip install -U \"ray[air]\" \"boto3\" \"ray\"\n",
    "! pip install -U protobuf==3.19.6 xgboost==1.3.3 xgboost-ray==0.1.15 pandas==1.5.3\n",
    "! pip install \"datasets\" \"evaluate\" \"accelerate==0.20.3\" \"transformers>=4.26.0\" \"torch>=1.12.0\" \"deepspeed==0.8.3\"\n",
    "! pip install pandas --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dbef74e",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe2cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ray\n",
    "from datasets import load_dataset\n",
    "import ray.data\n",
    "from transformers import AutoTokenizer\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import (\n",
    "    GPTJForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "import torch\n",
    "from ray.air import session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "798a7f5d",
   "metadata": {},
   "source": [
    "## Global variables definition for training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES DEFINITION\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "bucket = \"fm-ops-datasets\"\n",
    "use_gpu = True\n",
    "num_workers = 16\n",
    "cpus_per_worker = 8\n",
    "# Because the dataset is represented by a single large string, we will need to do some preprocessing\n",
    "block_size = 512\n",
    "# Since this example runs with multiple nodes, we need to persist checkpoints and other outputs to some external storage for access after training has completed.\n",
    "storage_path=f\"s3://{bucket}/checkpoints/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02241e75",
   "metadata": {},
   "source": [
    "## Connecting to Ray Cluster deployed in Amazon EKS\n",
    "\n",
    "Note that we are using the internal cluster DNS powered by CoreDNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496525e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connecting Ray with the cluster\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    address=\"ray://ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local:10001\",\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"datasets\",\n",
    "            \"evaluate\",\n",
    "            # Latest combination of accelerate==0.19.0 and transformers==4.29.0\n",
    "            # seems to have issues with DeepSpeed process group initialization,\n",
    "            # and will result in a batch_size validation problem.\n",
    "            # TODO(jungong) : get rid of the pins once the issue is fixed.\n",
    "            \"accelerate==0.20.3\",\n",
    "            \"transformers==4.26.0\",\n",
    "            \"torch>=1.12.0\",\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70b17dd2",
   "metadata": {},
   "source": [
    "# Importing Dataset\n",
    "\n",
    "We will be fine-tuning the model on the tiny_shakespeare dataset, comprised of 40,000 lines of Shakespeare from a variety of Shakespeareâ€™s plays. The aim will be to make the GPT-J model better at generating text in the style of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cb374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "print(\"Loading tiny_shakespeare dataset\")\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(current_dataset[\"train\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1e1e92c",
   "metadata": {},
   "source": [
    "## Pre processing dataset\n",
    "\n",
    "Note that the dataset is represented by a single line of large string, and needs some preprocessing. To do this, use the map_batches() API to apply transformation functions to batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1613869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ray Data for distributed preporcessing data ingestion\n",
    "ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "splitter = BatchMapper(split_text, batch_format=\"pandas\")\n",
    "tokenizer = BatchMapper(tokenize, batch_format=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7adc70",
   "metadata": {},
   "source": [
    "## Fine-tuning the model with Ray Train\n",
    "\n",
    "Configure Ray Trainâ€™s TorchTrainer to perform distributed fine-tuning of the model. Specify a train_loop_per_worker function, which defines the training logic to be distributed by Ray using Distributed Data Parallelism, which uses the PyTorch Distributed backend internally. Each worker has its own copy of the model, but operates on different data. At the end of each step, all the workers sync gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n",
    "    # Use the actual number of CPUs assigned by Ray\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(\n",
    "        session.get_trial_resources().bundles[-1].get(\"CPU\", 1)\n",
    "    )\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 4)\n",
    "    epochs = config.get(\"epochs\", 2)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "        },\n",
    "        \"bf16\": {\"enabled\": \"auto\"},\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        \"output\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"no\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        label_names=[\"input_ids\", \"attention_mask\"],\n",
    "        num_train_epochs=epochs,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "    disable_progress_bar()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f40fcf2",
   "metadata": {},
   "source": [
    "## Training Speed\n",
    "\n",
    "The preprocessed dataset has 1348 examples, and we have set the per-device batch size to 16.\n",
    "\n",
    "- With 16 g4dn.4xlarge nodes, the effective batch size was 256, which equals 85 steps per epoch. One epoch took ~2440 seconds (including initialization time).\n",
    "\n",
    "- With 32 g4dn.4xlarge nodes, the effective batch size was 512, which equals 43 steps per epoch. One epoch took ~1280 seconds (including initialization time).\n",
    "\n",
    "- With 4 g5.4xlarge nodes, the effective batch size was 64 (16 per device * 4 nodes). This results in approximately 21 steps per epoch (1348 examples / 64). The total running time was 3hr 53min 11s (or 13978 seconds), including initialization time.\n",
    "\n",
    "### Detailed Results\n",
    "\n",
    "Training finished at 2023-09-05 16:18:36. Total running time: 3hr 53min 11s (or 13978 seconds).\n",
    "\n",
    "```\n",
    "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "â”‚ Training result                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ time_this_iter_s           272.831 â”‚\n",
    "â”‚ time_total_s                 13978 â”‚\n",
    "â”‚ training_iteration             338 â”‚\n",
    "â”‚ epoch                            1 â”‚\n",
    "â”‚ learning_rate                    0 â”‚\n",
    "â”‚ loss                        0.0665 â”‚\n",
    "â”‚ step                           338 â”‚\n",
    "â”‚ train_loss                 0.13638 â”‚\n",
    "â”‚ train_runtime              13424.2 â”‚\n",
    "â”‚ train_samples_per_second     0.403 â”‚\n",
    "â”‚ train_steps_per_second       0.025 â”‚\n",
    "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "```\n",
    "\n",
    "With 4 g5.4xlarge nodes, the model completed one epoch in approximately 13978 seconds. Note that the number of steps (338) reported does not align perfectly with the calculated 21 steps per epoch based on batch size, which suggests that gradient accumulation or other settings might be in effect.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e47f645e",
   "metadata": {},
   "source": [
    "## Instantiate TorchTrainer\n",
    "\n",
    "After defining the training function, instantiate the TorchTrainer. Aside from the function, set the scaling_config to control the number of workers and amount of resources to use, and datasets(the preprocessed Ray Datasets) to use for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import TransformersTrainer\n",
    "from ray.air import RunConfig, ScalingConfig\n",
    "from ray.data.preprocessors import Chain\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    trainer_init_config={\n",
    "        \"batch_size\": 16,  # per device\n",
    "        \"epochs\": 1,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\"GPU\": 1, \"CPU\": cpus_per_worker},\n",
    "    ),\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"evaluation\": ray_datasets[\"validation\"]},\n",
    "    preprocessor=Chain(splitter, tokenizer),\n",
    "    run_config=RunConfig(storage_path=storage_path),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "841a3bd2",
   "metadata": {},
   "source": [
    "Finally, call the fit() method to start training with Ray Train. Save the Result object to a variable to access metrics and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This train will only kickstart the training in the Ray cluster, but we are not gonna use that\n",
    "results = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5682bdf5",
   "metadata": {},
   "source": [
    "## Close connection to Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537b732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4f8560f",
   "metadata": {},
   "source": [
    "# Ray's Job Submission API to submit a training job to a Ray cluster.\n",
    "\n",
    "In your code snippet, you're leveraging Ray's Job Submission API to remotely execute a training job on a Ray cluster. You initialize a JobSubmissionClient by connecting to the head node of the Ray cluster, then define a shell command (ray_training) that carries out several tasks: it removes any existing folder named fm-ops-eks, clones a specific git repository, grants executable permissions to a Python script (train_gptj.py), and then runs the script for training. Finally, you submit this shell command as a job to the Ray cluster using the submit_job method of the JobSubmissionClient.\n",
    "\n",
    "The reason for using Ray's Job Submission API instead of trainer.fit() directly in a Jupyter Notebook is that the latter doesn't allow you to see the logs directly within the notebook interface. Using the Job Submission API gives you more control over job monitoring and log inspection, which is especially useful for debugging and real-time monitoring of training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8edc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ray JOB Submission script\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "ray_client = JobSubmissionClient(\"http://ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local:8265\")\n",
    "ray_training = (\n",
    "    \"rm -rf fm-ops-eks && git clone https://github.com/lusoal/fm-ops-eks || true;\"\n",
    "    \"chmod +x fm-ops-eks/scripts/train_gptj.py && python fm-ops-eks/scripts/train_gptj.py\"\n",
    ")\n",
    "submission_id = ray_client.submit_job(entrypoint=ray_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop and deleting job that we used for testing\n",
    "ray_client.stop_job(submission_id)\n",
    "ray_client.delete_job(submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7bffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7766347",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76493ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb98489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "conda_env_path = os.environ.get('CONDA_PREFIX')\n",
    "print(f'Conda environment path: {conda_env_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2addd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda list | grep -i cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /usr/local/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! find / -iname '*cuda*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fded67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Devices count:\",torch.cuda.device_count())\n",
    "print(\"CUDA Version: \", torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
