####################################################################################
#This manifest launches a new RayCluster using GPU instances.
#When the job is completed the cluster resources are deleted for cost optimization
####################################################################################
apiVersion: v1
kind: Namespace
metadata:
  name: ray-job-finetuning

---

apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: ray-job-finetuning
  namespace: ray-job-finetuning
spec:
  # submissionMode specifies how RayJob submits the Ray job to the RayCluster.
  # The default value is "K8sJobMode", meaning RayJob will submit the Ray job via a submitter Kubernetes Job.
  # The alternative value is "HTTPMode", indicating that KubeRay will submit the Ray job by sending an HTTP request to the RayCluster.
  # submissionMode: "K8sJobMode"
  #entrypoint: "aws s3 cp s3://data-on-eks-genai/scripts/train_script.py train_script.py || true;chmod +x train_script.py && python train_script.py"
  entrypoint: "chmod +x train_script.py && python train_script.py"
  #entrypoint: ""
  # shutdownAfterJobFinishes specifies whether the RayCluster should be deleted after the RayJob finishes. Default is false.
  #shutdownAfterJobFinishes: true

  # ttlSecondsAfterFinished specifies the number of seconds after which the RayCluster will be deleted after the RayJob finishes.
  #ttlSecondsAfterFinished: 60

  # activeDeadlineSeconds is the duration in seconds that the RayJob may be active before
  # KubeRay actively tries to terminate the RayJob; value must be positive integer.
  # activeDeadlineSeconds: 120

  # RuntimeEnvYAML represents the runtime environment configuration provided as a multi-line YAML string.
  # See https://docs.ray.io/en/latest/ray-core/handling-dependencies.html for details.
  # (New in KubeRay version 1.0.)
  runtimeEnvYAML: |
    pip:
      - ray
      - transformers
      - datasets
      - numpy
      - evaluate
      - boto3

    # env_vars:
    #   counter_name: "test_counter"

  # Suspend specifies whether the RayJob controller should create a RayCluster instance.
  # If a job is applied with the suspend field set to true, the RayCluster will not be created and we will wait for the transition to false.
  # If the RayCluster is already created, it will be deleted. In the case of transition to false, a new RayCluster will be created.
  # suspend: true

  # rayClusterSpec specifies the RayCluster instance to be created by the RayJob controller.
  rayClusterSpec:
    rayVersion: '2.11.0' # should match the Ray version in the image of the containers
    enableInTreeAutoscaling: true 
    # Ray head pod template
    headGroupSpec:
      # The `rayStartParams` are used to configure the `ray start` command.
      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
      rayStartParams:
        dashboard-host: '0.0.0.0'
      #pod template
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.11.0.a464b6-py311-cpu
              resources:
                limits:
                  cpu: "12"
                  memory: "16Gi"
                requests:
                  cpu: "12"
                  memory: "16Gi"
              volumeMounts:
                - mountPath: /tmp/ray
                  name: log-volume
                - mountPath: /mnt/cluster_storage
                  name: ray-job-volume
                - mountPath: /home/ray/script
                  name: training-script
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              command: ["wget https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip && unzip awscli-exe-linux-x86_64.zip && sudo ./aws/install;aws s3 cp s3://data-on-eks-genai/scripts/train_script.py train_script.py || true;chmod +x train_script.py && python train_script.py"]
          volumes:
            - name: log-volume
              emptyDir: {}
            - name: ray-job-volume
              hostPath:
                path: /tmp/
            - name: training-script
              configMap:
                name: "finetuning-cm"
                items:
                  - key: train_script.py
                    path: train_script.py

    workerGroupSpecs:
      # the pod replicas in this group typed worker
      - replicas: 2
        minReplicas: 2
        maxReplicas: 10
        # logical group name, for this called small-group, also can be functional
        groupName: gpu-group
        # The `rayStartParams` are used to configure the `ray start` command.
        # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
        # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
        rayStartParams: {}
        #pod template
        template:
          spec:
            containers:
              - name: ray-worker # must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc'
                image: rayproject/ray-ml:2.11.0.a464b6-py311-gpu
                lifecycle:
                  preStop:
                    exec:
                      command: [ "/bin/sh","-c","ray stop" ]
                resources:
                  limits:
                    nvidia.com/gpu: 1
                    cpu: "8"
                    memory: "50Gi"
                    ephemeral-storage: "200Gi"
                  requests:
                    nvidia.com/gpu: 1
                    cpu: "8"
                    memory: "50Gi"
                    ephemeral-storage: "200Gi"
                volumeMounts:
                  - mountPath: /tmp/ray
                    name: log-volume
                  - mountPath: /mnt/cluster_storage
                    name: ray-job-volume
                command: ["wget https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip && unzip awscli-exe-linux-x86_64.zip && sudo ./aws/install"]    
                env:
                  - name: RAY_LOG_TO_STDERR
                    value: "1"
                  - name: CUDA_HOME
                    value: "/usr/local/cuda"




###########################################################################
#Add the train_script.py into the next configmap
###########################################################################
---
apiVersion: v1
data:
  train_script.py: |
    import os
    import numpy as np
    import evaluate
    from datasets import load_dataset
    from transformers import (
        Trainer,
        TrainingArguments,
        AutoTokenizer,
        AutoModelForSequenceClassification,
    )

    import ray.train.huggingface.transformers
    from ray.train import ScalingConfig, RunConfig
    from ray.train.torch import TorchTrainer

    # Variables
    s3_name_checkpoints = "data-on-eks-genai"
    storage_path=f"s3://{s3_name_checkpoints}/checkpoints/"

    def train_func():
        # Datasets
        dataset = load_dataset("yelp_review_full") # This is the dataset that we are using for train
        tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

        def tokenize_function(examples):
            return tokenizer(examples["text"], padding="max_length", truncation=True)

        small_train_dataset = (
            dataset["train"].select(range(1000)).map(tokenize_function, batched=True)
        )
        small_eval_dataset = (
            dataset["test"].select(range(1000)).map(tokenize_function, batched=True)
        )

        # Model
        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-cased", num_labels=5
        )

        # Evaluation Metrics
        metric = evaluate.load("accuracy")

        def compute_metrics(eval_pred):
            logits, labels = eval_pred
            predictions = np.argmax(logits, axis=-1)
            return metric.compute(predictions=predictions, references=labels)

        # Hugging Face Trainer
        training_args = TrainingArguments(
            output_dir="test_trainer",
            evaluation_strategy="epoch",
            save_strategy="epoch",
            report_to="none",
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=small_train_dataset,
            eval_dataset=small_eval_dataset,
            compute_metrics=compute_metrics,
        )

        # [2] Report Metrics and Checkpoints to Ray Train
        # ===============================================
        callback = ray.train.huggingface.transformers.RayTrainReportCallback()
        trainer.add_callback(callback)

        # [3] Prepare Transformers Trainer
        # ================================
        trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)

        # Start Training
        trainer.train()


    # [4] Define a Ray TorchTrainer to launch `train_func` on all workers
    # ===================================================================
    ray_trainer = TorchTrainer(
        train_func,
        scaling_config=ScalingConfig(num_workers=2, use_gpu=True),
        run_config=RunConfig(
            storage_path=storage_path,
            name="bert_experiment",
        )
        # [4a] If running in a multi-node cluster, this is where you
        # should configure the run's persistent storage that is accessible
        # across all worker nodes.
        # run_config=ray.train.RunConfig(storage_path="s3://..."),
    )

    ray_trainer.fit()

kind: ConfigMap
metadata:
  name: finetuning-cm
  namespace: ray-job-finetuning



