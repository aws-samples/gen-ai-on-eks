{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788cd78-63e9-4935-a2db-03a41b2966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets ray[default] ray[tune] ray[serve] boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6c99aff8-286a-47d3-ac14-d841f5c2406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_gpt2_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_gpt2_script.py\n",
    "import os\n",
    "import logging\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "import ray\n",
    "from ray.train.huggingface.transformers import RayTrainReportCallback, prepare_trainer\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Variables\n",
    "s3_bucket_name = \"YOUR_BUCKET_CREATED_BY_TERRAFORM\"\n",
    "storage_path = f\"s3://{s3_bucket_name}/checkpoints/\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(examples):\n",
    "    output = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": output.input_ids.long(),\n",
    "        \"attention_mask\": output.attention_mask.long(),\n",
    "        \"labels\": output.input_ids.clone()\n",
    "    }\n",
    "\n",
    "# Define the full training function\n",
    "def train_func():\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        # Ensure that predictions and labels are numpy arrays of type int32\n",
    "        predictions = predictions.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        labels = labels.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer_gpt2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        max_steps=100,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16\n",
    "    )\n",
    "\n",
    "    train_dataset = ray.train.get_dataset_shard(\"train\").iter_torch_batches(batch_size=1)\n",
    "    eval_dataset = ray.train.get_dataset_shard(\"validation\").iter_torch_batches(batch_size=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=True, logging_level=logging.DEBUG, ignore_reinit_error=True)\n",
    "    train_split = \"train[:1%]\"\n",
    "    validation_split = \"validation[:1%]\"\n",
    "    hf_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={'train': train_split, 'validation': validation_split})\n",
    "    processed_ds = hf_datasets.map(preprocess, batched=True, batch_size=1000)\n",
    "    processed_ds = processed_ds.remove_columns(\"text\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(processed_ds[\"train\"])\n",
    "    ray_eval_ds = ray.data.from_huggingface(processed_ds[\"validation\"])\n",
    "\n",
    "    logging.info(\"Configuring Ray Trainer...\")\n",
    "    ray_trainer = TorchTrainer(\n",
    "        train_func,\n",
    "        scaling_config=ScalingConfig(num_workers=10, use_gpu=True),\n",
    "        datasets={\"train\": ray_train_ds, \"validation\": ray_eval_ds},\n",
    "        run_config=RunConfig(storage_path=storage_path, name=\"gpt2_experiment\")\n",
    "    )\n",
    "    logging.info(\"Starting the Ray training process...\")\n",
    "    result = ray_trainer.fit()\n",
    "    logging.info(\"Ray training process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "26b27725-f296-4887-bb3f-f66a6049cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 bucket definition and upload of the training script\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240423160619841200000004\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./train_gpt2_script.py\", s3_name_checkpoints, \"scripts/train_gpt2_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2e0a8af8-7226-49db-be7b-a6ab2abdb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 22:36:21,741\tDEBUG utils.py:655 -- Using API server address http://ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local:8265.\n",
      "2024-04-25 22:36:21,754\tDEBUG validation.py:197 -- Rewrote runtime_env `pip` field from ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'] to {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False}.\n",
      "2024-04-25 22:36:21,755\tDEBUG validation.py:197 -- Rewrote runtime_env `pip` field from {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False} to {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False}.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Submitting Training script to Ray\n",
    "ray_train_address = \"ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local\"\n",
    "ray_client = JobSubmissionClient(f\"http://{ray_train_address}:8265\")\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240423160619841200000004\"\n",
    "train_dependencies = [\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"boto3\",\n",
    "    \"numpy\",\n",
    "    \"evaluate\"\n",
    "]\n",
    "\n",
    "submission_id = ray_client.submit_job(\n",
    "    # Entrypoint shell command to execute\n",
    "    entrypoint=(\n",
    "        f\"rm -rf train_gpt2_script.py && aws s3 cp s3://{s3_name_checkpoints}/scripts/train_gpt2_script.py train_gpt2_script.py || true;\"\n",
    "        \"chmod +x train_gpt2_script.py && python train_gpt2_script.py\"\n",
    "    ),\n",
    "    runtime_env={\n",
    "        \"pip\": train_dependencies\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49f46f1-ac64-4b02-900f-40ced30b1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:23:42,950 - INFO - Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-04-25 15:23:42,950 - INFO - NumExpr defaulting to 8 threads.\n",
      "ray, version 2.11.0\n",
      "\u001b[0mPython 3.11.8\n"
     ]
    }
   ],
   "source": [
    "! ray --version && python --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
