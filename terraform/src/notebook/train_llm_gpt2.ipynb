{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788cd78-63e9-4935-a2db-03a41b2966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets ray[default] ray[tune] ray[serve] boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99aff8-286a-47d3-ac14-d841f5c2406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_gpt2_script.py\n",
    "import os\n",
    "import logging\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "import ray\n",
    "from ray.train.huggingface.transformers import RayTrainReportCallback, prepare_trainer\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Variables\n",
    "s3_bucket_name = \"datasets-checkpoints20240409144007926200000004\"\n",
    "storage_path = f\"s3://{s3_bucket_name}/checkpoints/\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(examples):\n",
    "    output = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": output.input_ids.long(),\n",
    "        \"attention_mask\": output.attention_mask.long(),\n",
    "        \"labels\": output.input_ids.clone()\n",
    "    }\n",
    "\n",
    "# Define the full training function\n",
    "def train_func():\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        # Ensure that predictions and labels are numpy arrays of type int32\n",
    "        predictions = predictions.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        labels = labels.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer_gpt2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        max_steps=100,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16\n",
    "    )\n",
    "\n",
    "    train_dataset = ray.train.get_dataset_shard(\"train\").iter_torch_batches(batch_size=1)\n",
    "    eval_dataset = ray.train.get_dataset_shard(\"validation\").iter_torch_batches(batch_size=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=True, logging_level=logging.DEBUG, ignore_reinit_error=True)\n",
    "    train_split = \"train[:1%]\"\n",
    "    validation_split = \"validation[:1%]\"\n",
    "    hf_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={'train': train_split, 'validation': validation_split})\n",
    "    processed_ds = hf_datasets.map(preprocess, batched=True, batch_size=1000)\n",
    "    processed_ds = processed_ds.remove_columns(\"text\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(processed_ds[\"train\"])\n",
    "    ray_eval_ds = ray.data.from_huggingface(processed_ds[\"validation\"])\n",
    "\n",
    "    logging.info(\"Configuring Ray Trainer...\")\n",
    "    ray_trainer = TorchTrainer(\n",
    "        train_func,\n",
    "        scaling_config=ScalingConfig(num_workers=10, use_gpu=True),\n",
    "        datasets={\"train\": ray_train_ds, \"validation\": ray_eval_ds},\n",
    "        run_config=RunConfig(storage_path=storage_path, name=\"gpt2_experiment\")\n",
    "    )\n",
    "    logging.info(\"Starting the Ray training process...\")\n",
    "    result = ray_trainer.fit()\n",
    "    logging.info(\"Ray training process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26b27725-f296-4887-bb3f-f66a6049cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 bucket definition and upload of the training script\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240409144007926200000004\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./train_gpt2_script.py\", s3_name_checkpoints, \"scripts/train_gpt2_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0a8af8-7226-49db-be7b-a6ab2abdb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Submitting Training script to Ray\n",
    "ray_train_address = \"ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local\"\n",
    "ray_client = JobSubmissionClient(f\"http://{ray_train_address}:8265\")\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240409144007926200000004\"\n",
    "train_dependencies = [\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"boto3\",\n",
    "    \"numpy\",\n",
    "    \"evaluate\"\n",
    "]\n",
    "\n",
    "submission_id = ray_client.submit_job(\n",
    "    # Entrypoint shell command to execute\n",
    "    entrypoint=(\n",
    "        f\"rm -rf train_gpt2_script.py && aws s3 cp s3://{s3_name_checkpoints}/scripts/train_gpt2_script.py train_gpt2_script.py || true;\"\n",
    "        \"chmod +x train_gpt2_script.py && python train_gpt2_script.py\"\n",
    "    ),\n",
    "    runtime_env={\n",
    "        \"pip\": train_dependencies\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f46f1-ac64-4b02-900f-40ced30b1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ray --version && python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7a6c9-c4b2-49b7-a499-65838125c8ff",
   "metadata": {},
   "source": [
    "# Finetuned model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f9a8b-0c14-40c5-a579-efeabc0e080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    # Listing all objects within the base folder\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    # Sorting to find the latest\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    # Download files from the latest checkpoint\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):  # Skip directories\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "bucket_name = \"datasets-checkpoints20240409144007926200000004\"\n",
    "base_folder = \"checkpoints/gpt2_experiment/TorchTrainer_fbeef_00000_0_2024-05-17_01-18-04/\" # Can see what folder in Ray train Logs\n",
    "local_directory = \"./latest_model_checkpoint_gpt2\"\n",
    "\n",
    "download_latest_checkpoint(bucket_name, base_folder, local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ff16e-1f95-4246-944c-e1d97fed3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Reduce verbosity to avoid unnecessary logs during model loading\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, max_length=100):\n",
    "    # Encode the input prompt and move to the appropriate device\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    # Generate text using the model with adjusted settings for completeness\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        num_beams=5, \n",
    "        early_stopping=False,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        do_sample=True\n",
    "    )\n",
    "    print(outputs)\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Load tokenizer and model\n",
    "local_directory = \"./latest_model_checkpoint_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(local_directory)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Sample prompt for generation\n",
    "sample_prompt = \"What are the major achievements of Marie Curie?\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(sample_prompt, tokenizer, model)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fc142-bb6a-4c4d-b91d-01ef0afc7026",
   "metadata": {},
   "source": [
    "# Serving Script\n",
    "\n",
    "Serving script will be created and uploaded to S3 in ZIP format, then we will generate a PreSigned URL to use in the RayService CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bd8d3-0c56-4d03-91f8-52157bd8f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpt2_serve_script.py\n",
    "import os\n",
    "import boto3\n",
    "import ray\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import logging\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class TextGenerationDeployment:\n",
    "    def __init__(self, bucket_name, base_folder):\n",
    "        logger = logging.getLogger(\"ray.serve\")\n",
    "        logger.info(\"Inside TextGenerationDeployment init\")\n",
    "        local_directory = \"./latest_model_checkpoint\"\n",
    "        download_latest_checkpoint(bucket_name, base_folder, local_directory)\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(local_directory)\n",
    "        self.model.eval()\n",
    "        self.logger = logger\n",
    "        self.logger.info(\"Model Loading complete\")\n",
    "\n",
    "    def generate_text(self,prompt):\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        outputs = self.model.generate(\n",
    "            inputs, \n",
    "            max_length=100, \n",
    "            num_beams=5, \n",
    "            early_stopping=False,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "        # Decode and return the generated text\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        self.logger.info(\"Prompt\",prompt)\n",
    "        self.logger.info(\"Response\",generated_text)\n",
    "        return generated_text\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        body = await request.json()\n",
    "        prompt = body['prompt']\n",
    "        generated_text = self.generate_text(prompt)\n",
    "        return generated_text\n",
    "        \n",
    "bucket_name = \"datasets-checkpoints20240409144007926200000004\"\n",
    "base_folder = \"checkpoints/gpt2_experiment/TorchTrainer_fbeef_00000_0_2024-05-17_01-18-04/\"\n",
    "text_generate_deployment = TextGenerationDeployment.bind(bucket_name=bucket_name, base_folder=base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e45c4d2-5725-41ef-bff6-4e4e37432ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket_name = \"datasets-checkpoints20240409144007926200000004\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./gpt2_serve_script.py\", bucket_name, \"scripts/gpt2_serve_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855c88e-4763-4adf-aa0d-2ecc948fb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket = \"datasets-checkpoints20240409144007926200000004\"\n",
    "\n",
    "with ZipFile('./gpt2_finetuned.zip', 'w') as zip_object:\n",
    "    zip_object.write('./gpt2_serve_script.py')\n",
    "\n",
    "s3_client.upload_file(\"./gpt2_finetuned.zip\", bucket, \"gpt2_finetuned.zip\")\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    'get_object',\n",
    "    Params={'Bucket': bucket, 'Key': \"gpt2_finetuned.zip\"},\n",
    "    ExpiresIn=7200\n",
    ")\n",
    "\n",
    "print(\"Pre-signed URL:\", presigned_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe73af-e37c-4d56-b925-3ac6e3db8a84",
   "metadata": {},
   "source": [
    "## Sending Request to Finetuned GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f0d5026-8f41-44f4-a3bb-799875a46b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "904671d3-f07b-4649-96b4-c17c963dc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ray-svc-gpt2-head-svc.ray-svc-gpt2.svc.cluster.local:8000/gpt2_generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "addbe579-a323-4ecb-8a90-134df55c5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = json.dumps({\"prompt\": prompt})\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return \"Error:\", response.status_code, response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84960bb8-7620-45b7-9da7-0a9f40e894b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What are the major achievements of Marie Curie?\",\n",
    "    \"When did the second world war happen?\",\n",
    "    \"What is the capital of United Kingdom?\"\n",
    "    \n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generate_text(prompt)\n",
    "    print(result)\n",
    "    print()  # Adding a newline for better readability between results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5494b-1637-4673-9c57-b3a26b30a8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
