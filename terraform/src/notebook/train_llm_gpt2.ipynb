{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788cd78-63e9-4935-a2db-03a41b2966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets ray[default] ray[tune] ray[serve] boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6c99aff8-286a-47d3-ac14-d841f5c2406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_gpt2_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_gpt2_script.py\n",
    "import os\n",
    "import logging\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "import ray\n",
    "from ray.train.huggingface.transformers import RayTrainReportCallback, prepare_trainer\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Variables\n",
    "s3_bucket_name = \"datasets-checkpoints20240423160619841200000004\"\n",
    "storage_path = f\"s3://{s3_bucket_name}/checkpoints/\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess(examples):\n",
    "    output = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": output.input_ids.long(),\n",
    "        \"attention_mask\": output.attention_mask.long(),\n",
    "        \"labels\": output.input_ids.clone()\n",
    "    }\n",
    "\n",
    "# Define the full training function\n",
    "def train_func():\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        # Ensure that predictions and labels are numpy arrays of type int32\n",
    "        predictions = predictions.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        labels = labels.flatten().astype(np.int32)  # Flatten and convert to int32\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer_gpt2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        max_steps=100,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16\n",
    "    )\n",
    "\n",
    "    train_dataset = ray.train.get_dataset_shard(\"train\").iter_torch_batches(batch_size=1)\n",
    "    eval_dataset = ray.train.get_dataset_shard(\"validation\").iter_torch_batches(batch_size=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.train()\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=True, logging_level=logging.DEBUG, ignore_reinit_error=True)\n",
    "    train_split = \"train[:1%]\"\n",
    "    validation_split = \"validation[:1%]\"\n",
    "    hf_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={'train': train_split, 'validation': validation_split})\n",
    "    processed_ds = hf_datasets.map(preprocess, batched=True, batch_size=1000)\n",
    "    processed_ds = processed_ds.remove_columns(\"text\")\n",
    "\n",
    "    ray_train_ds = ray.data.from_huggingface(processed_ds[\"train\"])\n",
    "    ray_eval_ds = ray.data.from_huggingface(processed_ds[\"validation\"])\n",
    "\n",
    "    logging.info(\"Configuring Ray Trainer...\")\n",
    "    ray_trainer = TorchTrainer(\n",
    "        train_func,\n",
    "        scaling_config=ScalingConfig(num_workers=10, use_gpu=True),\n",
    "        datasets={\"train\": ray_train_ds, \"validation\": ray_eval_ds},\n",
    "        run_config=RunConfig(storage_path=storage_path, name=\"gpt2_experiment\")\n",
    "    )\n",
    "    logging.info(\"Starting the Ray training process...\")\n",
    "    result = ray_trainer.fit()\n",
    "    logging.info(\"Ray training process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "26b27725-f296-4887-bb3f-f66a6049cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 bucket definition and upload of the training script\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240423160619841200000004\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./train_gpt2_script.py\", s3_name_checkpoints, \"scripts/train_gpt2_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2e0a8af8-7226-49db-be7b-a6ab2abdb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 22:36:21,741\tDEBUG utils.py:655 -- Using API server address http://ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local:8265.\n",
      "2024-04-25 22:36:21,754\tDEBUG validation.py:197 -- Rewrote runtime_env `pip` field from ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'] to {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False}.\n",
      "2024-04-25 22:36:21,755\tDEBUG validation.py:197 -- Rewrote runtime_env `pip` field from {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False} to {'packages': ['transformers', 'datasets', 'boto3', 'numpy', 'evaluate'], 'pip_check': False}.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Submitting Training script to Ray\n",
    "ray_train_address = \"ray-cluster-train-kuberay-head-svc.ray-cluster-train.svc.cluster.local\"\n",
    "ray_client = JobSubmissionClient(f\"http://{ray_train_address}:8265\")\n",
    "s3_name_checkpoints = \"datasets-checkpoints20240423160619841200000004\"\n",
    "train_dependencies = [\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"boto3\",\n",
    "    \"numpy\",\n",
    "    \"evaluate\"\n",
    "]\n",
    "\n",
    "submission_id = ray_client.submit_job(\n",
    "    # Entrypoint shell command to execute\n",
    "    entrypoint=(\n",
    "        f\"rm -rf train_gpt2_script.py && aws s3 cp s3://{s3_name_checkpoints}/scripts/train_gpt2_script.py train_gpt2_script.py || true;\"\n",
    "        \"chmod +x train_gpt2_script.py && python train_gpt2_script.py\"\n",
    "    ),\n",
    "    runtime_env={\n",
    "        \"pip\": train_dependencies\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49f46f1-ac64-4b02-900f-40ced30b1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-25 15:23:42,950 - INFO - Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-04-25 15:23:42,950 - INFO - NumExpr defaulting to 8 threads.\n",
      "ray, version 2.11.0\n",
      "\u001b[0mPython 3.11.8\n"
     ]
    }
   ],
   "source": [
    "! ray --version && python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7a6c9-c4b2-49b7-a499-65838125c8ff",
   "metadata": {},
   "source": [
    "# Finetuned model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "612f9a8b-0c14-40c5-a579-efeabc0e080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/config.json to ./latest_model_checkpoint_gpt2/config.json\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/generation_config.json to ./latest_model_checkpoint_gpt2/generation_config.json\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/model.safetensors to ./latest_model_checkpoint_gpt2/model.safetensors\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/optimizer.pt to ./latest_model_checkpoint_gpt2/optimizer.pt\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_0.pth to ./latest_model_checkpoint_gpt2/rng_state_0.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_1.pth to ./latest_model_checkpoint_gpt2/rng_state_1.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_2.pth to ./latest_model_checkpoint_gpt2/rng_state_2.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_3.pth to ./latest_model_checkpoint_gpt2/rng_state_3.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_4.pth to ./latest_model_checkpoint_gpt2/rng_state_4.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_5.pth to ./latest_model_checkpoint_gpt2/rng_state_5.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_6.pth to ./latest_model_checkpoint_gpt2/rng_state_6.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_7.pth to ./latest_model_checkpoint_gpt2/rng_state_7.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_8.pth to ./latest_model_checkpoint_gpt2/rng_state_8.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/rng_state_9.pth to ./latest_model_checkpoint_gpt2/rng_state_9.pth\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/scheduler.pt to ./latest_model_checkpoint_gpt2/scheduler.pt\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/trainer_state.json to ./latest_model_checkpoint_gpt2/trainer_state.json\n",
      "Downloaded: checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/checkpoint_000044/checkpoint/training_args.bin to ./latest_model_checkpoint_gpt2/training_args.bin\n",
      "All files from the latest checkpoint are downloaded.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    # Listing all objects within the base folder\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    # Sorting to find the latest\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    # Download files from the latest checkpoint\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):  # Skip directories\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "bucket_name = \"datasets-checkpoints20240423160619841200000004\"\n",
    "base_folder = \"checkpoints/gpt2_experiment/TorchTrainer_4790d_00000_0_2024-04-25_15-36-38/\" # Can see what folder in Ray train Logs\n",
    "local_directory = \"./latest_model_checkpoint_gpt2\"\n",
    "\n",
    "download_latest_checkpoint(bucket_name, base_folder, local_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2a8ff16e-1f95-4246-944c-e1d97fed3d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c6a088ec844f14b4dc6aaa8fc07059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edc9890cc8d4976bb6a98c7a31f85c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3399a8cbacb1499b8a28d45e0446fa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b1bbd5e459495e808b703c7376304c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65a5a5c6fb843ecb5fe2c1cc67e323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Model loaded successfully!\n",
      "Generated text: The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing. The service was excellent and the atmosphere was warm and inviting. I would highly recommend this place to anyone who is looking for a quick lunch or dinner fix.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, max_length=512):\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Load tokenizer and model\n",
    "local_directory = \"./latest_model_checkpoint_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(local_directory)\n",
    "\n",
    "model.eval()\n",
    "print(\"GPT-2 Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdf10d-44a1-4c72-baf9-ab30d9b62acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Sample prompt for generation\n",
    "sample_prompt = \"What were the main causes of the American Civil War?\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(sample_prompt, tokenizer, model)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
