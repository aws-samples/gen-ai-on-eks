{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1602480-5edd-4ef5-90a5-f27861a163e8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with BERT on Yelp Reviews\n",
    "\n",
    "This Jupyter Notebook provides a detailed example of how to create all the scripts and artifacts needed for train a BERT model for sentiment analysis using Yelp review data. We will go through all steps need to create a pipeline for production purposes\n",
    "\n",
    "## What is BERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training. Developed by Google, BERT's key innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
    "\n",
    "This is particularly effective in understanding the context of a word based on all of its surroundings (left and right of the word).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91fd493-48e5-43e8-bbde-7190bd63ed67",
   "metadata": {},
   "source": [
    "### Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a239712a-ee11-4da0-ba64-1058fdf679d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray in /opt/conda/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.34.90)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from ray) (8.1.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from ray) (3.9.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from ray) (4.21.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ray) (24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.11/site-packages (from ray) (4.25.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ray) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.11/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.11/site-packages (from ray) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from ray) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.90 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.34.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.90->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.90->boto3) (2.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2024.2.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.90->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"ray\" \"transformers\" \"datasets\" \"numpy\" \"evaluate\" \"boto3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cef5e1d-ef1b-4447-86f5-a86c20ccfb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA and GPU is available:\n",
    "\n",
    "def checking_cuda():\n",
    "    import torch\n",
    "    \n",
    "    # Checks if a GPU is available and identifies it\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "checking_cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bca91-547e-4fdc-baa3-4d397e4db518",
   "metadata": {},
   "source": [
    "# Training Script\n",
    "\n",
    "This Training script will be created and uploaded to S3 in ZIP format, then we will generate a PreSigned URL to use in the RayJob CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f79c4fca-75c4-4c7e-a085-814517a7693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script.py\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "import ray.train.huggingface.transformers\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Variables\n",
    "s3_name_checkpoints = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "storage_path=f\"s3://{s3_name_checkpoints}/checkpoints/\"\n",
    "\n",
    "def train_func():\n",
    "    # Datasets\n",
    "    dataset = load_dataset(\"yelp_review_full\") # This is the dataset that we are using for train\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    small_train_dataset = (\n",
    "        dataset[\"train\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    )\n",
    "    small_eval_dataset = (\n",
    "        dataset[\"test\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-cased\", num_labels=5\n",
    "    )\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Hugging Face Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train_dataset,\n",
    "        eval_dataset=small_eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # [2] Report Metrics and Checkpoints to Ray Train\n",
    "    # ===============================================\n",
    "    callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "    # [3] Prepare Transformers Trainer\n",
    "    # ================================\n",
    "    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n",
    "\n",
    "    # Start Training\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "# [4] Define a Ray TorchTrainer to launch `train_func` on all workers\n",
    "# ===================================================================\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        storage_path=storage_path,\n",
    "        name=\"bert_experiment\",\n",
    "    )\n",
    "    # [4a] If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "\n",
    "ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d357fd6-8de7-4c34-938b-cb1dbdba5633",
   "metadata": {},
   "source": [
    "## Creating pre-signed URL for train_script.py\n",
    "\n",
    "We will need this pre-signed URL to run the training job into the ephimeral Ray Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86cef7a3-0774-4634-916f-e0679863f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-signed URL: https://data-on-eks-genai.s3.amazonaws.com/bert_finetuned.zip?AWSAccessKeyId=ASIAZXNCLXBU4ZBY5KUR&Signature=Rt%2F4QinDhVWksB4kHtZ4m6%2FfGU0%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEG8aCXVzLXdlc3QtMiJHMEUCIGZA4NZ%2BC3%2BKN7IZTsqiiKNT8m8n16I3ojOn9NKC7wybAiEAjBvc%2FDB%2B%2F8uBuCzfVIHKLwGHgSQHJqx7JQ0lLwxpxVUqlAUI%2BP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw2Njg3NDQ3OTIxNjkiDDNF1YGERJ00XawNkSroBDoDq4Q%2BkzzsSw6JS30YmA1BaheirYnK3isVCkG3uUS5SeLrD0yKMOllzDZyD6gVlaYXybsdTI5zapcUP9fARuANKuMeKf7osq3tT5gCVX%2Fu0z2mLvHQuVE4PW%2FOGcjuHTxkKMCh3dWL4NBhQLZ6eS91UTYA5N4FjpQ678B2SZ%2Ft86y5%2FTmyK%2B4TI%2FxWsbX1ZnRDUgS0get4nBce%2FOJ2v29S90LsLrR7hLa673W3%2Bd5gnEFRqS%2B6TxGh6E0anuc0Do85GtotEkbevxB%2F3TPfKFFJhGT2uk5Pzllj3uD%2FYUqcvHKuuu%2B4CTwWxiNBthQ675ZkU0sx4vmwc1NqBnjiA2V6NSyXiKJan6NJC8XfZ8CEE5SxmZWVnihhdjWWsZV%2BBOfS%2BZ%2ByZVmEK7fG8xQRjEno5xItCAuf9NtzcuvDOQ2fe2OrebJfwam2nNvyL7YIK0yciDVJe9tf6zTCUyc26bMbPjmec%2BY2bb3uRsNhVl0UGCoLDLGA0Oo3hkWx6casprjZRv%2FAEtuVdSTzm0PUXJkMZrnwCsOjdKt4%2FU3j%2F7Jw1d7WMJETarJgbMcv2RpCTvI4oxBKjaEYeiXoIewYJpoIc6Kdfr7nNKYqYZyO9yKXNxr9KHBK6haUCfhi%2BXidn2BKgTou5ue1K8p%2F2TiM%2FFAbymA6dS90IxdnJWrixciLqFQIrQSlsj3TTlg9kg0%2BhlEFD9JHKzi95dXsuqC4XaWOdIVT9BBJNhf6ln74ouhIMNG1LTcF1ID4BmLChUy2GiO0eXMqJnp%2FQrJ8L925EQwq7AadtpeA4dxd4IeCQuAeqiKJfosDZ0EwzaWOswY6mwEV%2BX0rFB8UIU9fgUtehT0%2BZxyLQ94AG8Xgws6ZlykTzctdPdvern8XM6sqbk3BMvuPpmJSTN8nXIvfbNewO2f0y1qaexPx%2FZBPD9001Rfsi3DEFDLV9VmozkmSa%2BO8sz4T2HJBRfBrrTpDo7JiG80Ks7mbWqCWpY%2FlsN0j4q%2FUSfUeHwW67jo3JnP%2BoDinRlCCWjFu5RXrjifGQA%3D%3D&Expires=1717805533\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 bucket definition and upload of the training script\n",
    "s3_name_checkpoints = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "bucket_name = s3_name_checkpoints\n",
    "\n",
    "from zipfile import ZipFile\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "with ZipFile('./bert_train_script.zip', 'w') as zip_object:\n",
    "    zip_object.write('./train_script.py')\n",
    "\n",
    "s3_client.upload_file(\"./bert_train_script.zip\", bucket_name, \"/scripts/bert_train_script.zip\")\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    'get_object',\n",
    "    Params={'Bucket': bucket_name, 'Key': \"/scripts/bert_train_script.zip\"},\n",
    "    ExpiresIn=3600\n",
    ")\n",
    "\n",
    "print(\"Pre-signed URL:\", presigned_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f99f5-fb12-4e25-8f78-7d7621402d82",
   "metadata": {},
   "source": [
    "# Running the training job\n",
    "\n",
    "The pre-signed URL must be replaced into the file 03_ray_job_training_standalone_s3.yaml and then must be deployed with the following command:\n",
    "\n",
    "```bash\n",
    "kubectl apply -f 03_ray_job_training_standalone_s3.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09131ec-7d75-4223-a76a-1f6f76a29258",
   "metadata": {},
   "source": [
    "# Creating the serving Script\n",
    "\n",
    "Serving script will be created and uploaded to S3 in ZIP format, then we will generate a PreSigned URL to use in the RayService CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c1339fd-89de-4dd6-86d1-779e0b1e2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serve_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serve_script.py\n",
    "import os\n",
    "import boto3\n",
    "import ray\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class SentimentAnalysisDeployment:\n",
    "    def __init__(self, bucket_name, base_folder):\n",
    "        local_directory = \"./latest_model_checkpoint\"\n",
    "        download_latest_checkpoint(bucket_name, base_folder, local_directory)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(local_directory)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "            return probabilities\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        body = await request.json()\n",
    "        text = body['text']\n",
    "        probabilities = self.predict(text)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "        class_labels = [\"1 Star\", \"2 Stars\", \"3 Stars\", \"4 Stars\", \"5 Stars\"]\n",
    "        predicted_label = class_labels[predicted_class.item()]\n",
    "        probabilities_percent = [f\"{prob * 100:.2f}%\" for prob in probabilities[0]]\n",
    "        response = {\n",
    "            \"predicted_class\": predicted_label,\n",
    "            \"class_probabilities\": probabilities_percent\n",
    "        }\n",
    "        return response\n",
    "        \n",
    "bucket_name = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "base_folder = \"checkpoints/bert_experiment/\"\n",
    "sentiment_deployment = SentimentAnalysisDeployment.bind(bucket_name=bucket_name, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b314b4-d997-45b8-bca0-88f1d843a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./serve_script.py\", bucket_name, \"scripts/serve_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf484e33-061b-4285-a020-7135f5bb29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "with ZipFile('./bert_finetuned.zip', 'w') as zip_object:\n",
    "    zip_object.write('./serve_script.py')\n",
    "\n",
    "s3_client.upload_file(\"./bert_finetuned.zip\", bucket_name, \"/scripts/bert_finetuned.zip\")\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    'get_object',\n",
    "    Params={'Bucket': bucket_name, 'Key': \"/scripts/bert_finetuned.zip\"},\n",
    "    ExpiresIn=3600\n",
    ")\n",
    "\n",
    "print(\"Pre-signed URL:\", presigned_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03a7e7-f20d-452a-af15-1a2b975c632a",
   "metadata": {},
   "source": [
    "Replace the `01_ray_serve_bert_finetuned.yaml` file with the Presigned URL above and apply the manifest\n",
    "\n",
    "```bash\n",
    "kubectl apply -f ray_serve_manifests/01_ray_serve_bert_finetuned.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539db2b-ef93-4d55-a47c-38a8d04bf024",
   "metadata": {},
   "source": [
    "# Sending Request to Finetuned BERT, YELP Review (Optional step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e14fb61-4c92-4f4f-b6a6-9053786dbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98732165-8bb9-4e91-afb9-b25288ad2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ray-svc-bert-head-svc.ray-svc-bert.svc.cluster.local:8000/bert_predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09723ff9-69d4-4306-b46c-c8879513d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = json.dumps({\"text\": text})\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Error:\", response.status_code, response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94ef7add-c96a-4309-aa0c-85e0111c98cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.73%', '0.61%', '4.72%', '24.02%', '69.92%']}\n",
      "\n",
      "Review: I had a terrible experience. The food was bland and the service was too slow.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['79.51%', '16.45%', '2.21%', '1.11%', '0.72%']}\n",
      "\n",
      "Review: Quite a pleasant surprise! The dishes were creative and flavorful, and the staff was attentive.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.82%', '0.59%', '4.94%', '25.15%', '68.51%']}\n",
      "\n",
      "Review: Not worth the money. The food was mediocre at best and the ambiance wasn't as advertised.\n",
      "Sentiment Analysis Result: {'predicted_class': '2 Stars', 'class_probabilities': ['15.27%', '75.96%', '6.72%', '1.42%', '0.64%']}\n",
      "\n",
      "Review: An excellent choice for our anniversary dinner. We had a delightful evening with great food and service.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['1.09%', '0.53%', '4.60%', '24.22%', '69.56%']}\n",
      "\n",
      "Review: The service was below average and the waiter seemed uninterested.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['53.14%', '39.92%', '4.71%', '1.38%', '0.85%']}\n",
      "\n",
      "Review: Loved the vibrant atmosphere and the innovative menu choices. Will definitely return!\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.84%', '0.71%', '3.43%', '10.00%', '85.02%']}\n",
      "\n",
      "Review: It was just okay, not what I expected from the rave reviews.\n",
      "Sentiment Analysis Result: {'predicted_class': '2 Stars', 'class_probabilities': ['9.37%', '73.47%', '13.24%', '2.99%', '0.93%']}\n",
      "\n",
      "Review: The worst restaurant experience ever. Would not recommend to anyone.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['88.76%', '7.32%', '1.71%', '1.21%', '1.01%']}\n",
      "\n",
      "Review: Absolutely fantastic! Can't wait to go back and try more dishes.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.76%', '0.70%', '4.39%', '11.77%', '82.37%']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    \"The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing.\",\n",
    "    \"I had a terrible experience. The food was bland and the service was too slow.\",\n",
    "    \"Quite a pleasant surprise! The dishes were creative and flavorful, and the staff was attentive.\",\n",
    "    \"Not worth the money. The food was mediocre at best and the ambiance wasn't as advertised.\",\n",
    "    \"An excellent choice for our anniversary dinner. We had a delightful evening with great food and service.\",\n",
    "    \"The service was below average and the waiter seemed uninterested.\",\n",
    "    \"Loved the vibrant atmosphere and the innovative menu choices. Will definitely return!\",\n",
    "    \"It was just okay, not what I expected from the rave reviews.\",\n",
    "    \"The worst restaurant experience ever. Would not recommend to anyone.\",\n",
    "    \"Absolutely fantastic! Can't wait to go back and try more dishes.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for review in reviews:\n",
    "    result = get_sentiment(review)\n",
    "    results.append(result)\n",
    "    print(\"Review:\", review)\n",
    "    print(\"Sentiment Analysis Result:\", result)\n",
    "    print()  # Adding a newline for better readability between results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
