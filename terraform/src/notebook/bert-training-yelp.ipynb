{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1602480-5edd-4ef5-90a5-f27861a163e8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with BERT on Yelp Reviews\n",
    "\n",
    "This Jupyter Notebook provides a detailed example of how to train a BERT model for sentiment analysis using Yelp review data. We will go through all steps from loading and preprocessing the data to training the model and making predictions.\n",
    "\n",
    "## What is BERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training. Developed by Google, BERT's key innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
    "\n",
    "This is particularly effective in understanding the context of a word based on all of its surroundings (left and right of the word).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91fd493-48e5-43e8-bbde-7190bd63ed67",
   "metadata": {},
   "source": [
    "### Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a239712a-ee11-4da0-ba64-1058fdf679d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray in /opt/conda/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.34.90)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from ray) (8.1.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from ray) (3.9.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from ray) (4.21.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ray) (24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.11/site-packages (from ray) (4.25.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ray) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.11/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.11/site-packages (from ray) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from ray) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.90 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.34.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.90->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.90->boto3) (2.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2024.2.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.90->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"ray\" \"transformers\" \"datasets\" \"numpy\" \"evaluate\" \"boto3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cef5e1d-ef1b-4447-86f5-a86c20ccfb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA and GPU is available:\n",
    "\n",
    "def checking_cuda():\n",
    "    import torch\n",
    "    \n",
    "    # Checks if a GPU is available and identifies it\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "checking_cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bca91-547e-4fdc-baa3-4d397e4db518",
   "metadata": {},
   "source": [
    "# Training Script\n",
    "\n",
    "This Training script is built here in Jupyter Notebook but we submit it to Ray using Python, there are ways of doing that with the Ray Operator itself using RayJob CRD in Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f79c4fca-75c4-4c7e-a085-814517a7693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script.py\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "import ray.train.huggingface.transformers\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Variables\n",
    "s3_name_checkpoints = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "storage_path=f\"s3://{s3_name_checkpoints}/checkpoints/\"\n",
    "\n",
    "def train_func():\n",
    "    # Datasets\n",
    "    dataset = load_dataset(\"yelp_review_full\") # This is the dataset that we are using for train\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    small_train_dataset = (\n",
    "        dataset[\"train\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    )\n",
    "    small_eval_dataset = (\n",
    "        dataset[\"test\"].select(range(1000)).map(tokenize_function, batched=True)\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-cased\", num_labels=5\n",
    "    )\n",
    "\n",
    "    # Evaluation Metrics\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Hugging Face Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train_dataset,\n",
    "        eval_dataset=small_eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # [2] Report Metrics and Checkpoints to Ray Train\n",
    "    # ===============================================\n",
    "    callback = ray.train.huggingface.transformers.RayTrainReportCallback()\n",
    "    trainer.add_callback(callback)\n",
    "\n",
    "    # [3] Prepare Transformers Trainer\n",
    "    # ================================\n",
    "    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)\n",
    "\n",
    "    # Start Training\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "# [4] Define a Ray TorchTrainer to launch `train_func` on all workers\n",
    "# ===================================================================\n",
    "ray_trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        storage_path=storage_path,\n",
    "        name=\"bert_experiment\",\n",
    "    )\n",
    "    # [4a] If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "\n",
    "ray_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d357fd6-8de7-4c34-938b-cb1dbdba5633",
   "metadata": {},
   "source": [
    "# Submitting Training to Ray Cluster\n",
    "\n",
    "We have a Ray cluster already deployed that we are using for training, so we need to submit the training to Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ec000a9-0c21-4109-a509-0a2d8b999423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 bucket definition and upload of the training script\n",
    "s3_name_checkpoints = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./train_script.py\", s3_name_checkpoints, \"scripts/train_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18ee1693-145c-4483-ac85-67146072b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "# Submitting Training script to Ray\n",
    "ray_train_address = \"ray-cluster-train-kuberay-head-svc.ray-cluster-train-dev.svc.cluster.local\"\n",
    "ray_client = JobSubmissionClient(f\"http://{ray_train_address}:8265\")\n",
    "train_dependencies = [\n",
    "    \"ray\",\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"numpy\",\n",
    "    \"evaluate\",\n",
    "    \"boto3\"\n",
    "]\n",
    "\n",
    "submission_id = ray_client.submit_job(\n",
    "    # Entrypoint shell command to execute\n",
    "    entrypoint=(\n",
    "        f\"rm -rf train_script.py && aws s3 cp s3://{s3_name_checkpoints}/scripts/train_script.py train_script.py || true;\"\n",
    "        \"chmod +x train_script.py && python train_script.py\"\n",
    "    ),\n",
    "    runtime_env={\n",
    "        \"pip\": train_dependencies\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f99f5-fb12-4e25-8f78-7d7621402d82",
   "metadata": {},
   "source": [
    "## Check Ray Dashboard\n",
    "\n",
    "Before loading and testing your model, load the Ray Dashboard to see your job running\n",
    "\n",
    "```bash\n",
    "kubectl port-forward svc/ray-cluster-train-kuberay-head-svc 8265:8265 -nray-cluster-train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d44f69-d087-4cd1-b9e0-722e22f37db0",
   "metadata": {},
   "source": [
    "# Loading the model and doing local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dabd952e-9867-4f75-9716-60228393c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/config.json to ./latest_model_checkpoint/config.json\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/optimizer.pt to ./latest_model_checkpoint/optimizer.pt\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/pytorch_model.bin to ./latest_model_checkpoint/pytorch_model.bin\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/rng_state_0.pth to ./latest_model_checkpoint/rng_state_0.pth\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/scheduler.pt to ./latest_model_checkpoint/scheduler.pt\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/trainer_state.json to ./latest_model_checkpoint/trainer_state.json\n",
      "Downloaded: checkpoints/bert_experiment/TorchTrainer_60ca2_00000_0_2024-04-24_11-13-40/checkpoint_000002/checkpoint/training_args.bin to ./latest_model_checkpoint/training_args.bin\n",
      "All files from the latest checkpoint are downloaded.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    # Listing all objects within the base folder\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    # Sorting to find the latest\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    # Download files from the latest checkpoint\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):  # Skip directories\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "bucket_name = s3_name_checkpoints\n",
    "base_folder = \"checkpoints/bert_experiment/\"\n",
    "local_directory = \"./latest_model_checkpoint\"\n",
    "\n",
    "download_latest_checkpoint(bucket_name, base_folder, local_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97fcf0-a472-49c3-8f7f-f079e48c1175",
   "metadata": {},
   "source": [
    "# Testing the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87eda20e-5d93-4415-9070-803e78586a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Predicted class: 5 Stars\n",
      "Class probabilities: ['0.73%', '0.61%', '4.72%', '24.02%', '69.92%']\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the latest checkpoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def predict(text, tokenizer, model):\n",
    "    # Encode the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    return probabilities\n",
    "\n",
    "local_directory = \"./latest_model_checkpoint\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_directory)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Sample text for prediction\n",
    "sample_text = \"The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing.\"\n",
    "\n",
    "# Get prediction\n",
    "probabilities = predict(sample_text, tokenizer, model)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "# Map class indices to labels\n",
    "class_labels = {\n",
    "    0: \"1 Star\",\n",
    "    1: \"2 Stars\",\n",
    "    2: \"3 Stars\",\n",
    "    3: \"4 Stars\",\n",
    "    4: \"5 Stars\"\n",
    "}\n",
    "predicted_label = class_labels[predicted_class.item()]\n",
    "probabilities_percent = [f\"{prob * 100:.2f}%\" for prob in probabilities[0]]\n",
    "\n",
    "print(f\"Predicted class: {predicted_label}\")\n",
    "print(f\"Class probabilities: {probabilities_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09131ec-7d75-4223-a76a-1f6f76a29258",
   "metadata": {},
   "source": [
    "# Serving Script\n",
    "\n",
    "Serving script will be created and uploaded to S3 in ZIP format, then we will generate a PreSigned URL to use in the RayService CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c1339fd-89de-4dd6-86d1-779e0b1e2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serve_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serve_script.py\n",
    "import os\n",
    "import boto3\n",
    "import ray\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def download_latest_checkpoint(bucket_name, base_folder, local_directory):\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    checkpoints = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('/') and 'checkpoint' in key:\n",
    "                checkpoints.append(key)\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    latest_checkpoint = sorted(checkpoints)[-1]\n",
    "    print(\"Latest checkpoint:\", latest_checkpoint)\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=latest_checkpoint):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            local_file_path = os.path.join(local_directory, key[len(latest_checkpoint):])\n",
    "            if not key.endswith('/'):\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "                s3.download_file(bucket_name, key, local_file_path)\n",
    "                print(f'Downloaded: {key} to {local_file_path}')\n",
    "    print(\"All files from the latest checkpoint are downloaded.\")\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class SentimentAnalysisDeployment:\n",
    "    def __init__(self, bucket_name, base_folder):\n",
    "        local_directory = \"./latest_model_checkpoint\"\n",
    "        download_latest_checkpoint(bucket_name, base_folder, local_directory)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(local_directory)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "            return probabilities\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        body = await request.json()\n",
    "        text = body['text']\n",
    "        probabilities = self.predict(text)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "        class_labels = [\"1 Star\", \"2 Stars\", \"3 Stars\", \"4 Stars\", \"5 Stars\"]\n",
    "        predicted_label = class_labels[predicted_class.item()]\n",
    "        probabilities_percent = [f\"{prob * 100:.2f}%\" for prob in probabilities[0]]\n",
    "        response = {\n",
    "            \"predicted_class\": predicted_label,\n",
    "            \"class_probabilities\": probabilities_percent\n",
    "        }\n",
    "        return response\n",
    "        \n",
    "bucket_name = \"<REPLACE_WITH_YOUR_BUCKET_CREATED_BY_TERRAFORM>\"\n",
    "base_folder = \"checkpoints/bert_experiment/\"\n",
    "sentiment_deployment = SentimentAnalysisDeployment.bind(bucket_name=bucket_name, base_folder=base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b314b4-d997-45b8-bca0-88f1d843a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"./serve_script.py\", bucket_name, \"scripts/serve_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf484e33-061b-4285-a020-7135f5bb29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "with ZipFile('./bert_finetuned.zip', 'w') as zip_object:\n",
    "    zip_object.write('./serve_script.py')\n",
    "\n",
    "s3_client.upload_file(\"./bert_finetuned.zip\", bucket_name, \"bert_finetuned.zip\")\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    'get_object',\n",
    "    Params={'Bucket': bucket_name, 'Key': \"bert_finetuned.zip\"},\n",
    "    ExpiresIn=3600\n",
    ")\n",
    "\n",
    "print(\"Pre-signed URL:\", presigned_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03a7e7-f20d-452a-af15-1a2b975c632a",
   "metadata": {},
   "source": [
    "Replace the `01_ray_serve_bert_finetuned.yaml` file with the Presigned URL above and apply the manifest\n",
    "\n",
    "```bash\n",
    "kubectl apply -f ray_serve_manifests/01_ray_serve_bert_finetuned.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539db2b-ef93-4d55-a47c-38a8d04bf024",
   "metadata": {},
   "source": [
    "# Sending Request to Finetuned BERT, YELP Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e14fb61-4c92-4f4f-b6a6-9053786dbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98732165-8bb9-4e91-afb9-b25288ad2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ray-svc-bert-head-svc.ray-svc-bert.svc.cluster.local:8000/bert_predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09723ff9-69d4-4306-b46c-c8879513d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = json.dumps({\"text\": text})\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Error:\", response.status_code, response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94ef7add-c96a-4309-aa0c-85e0111c98cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.73%', '0.61%', '4.72%', '24.02%', '69.92%']}\n",
      "\n",
      "Review: I had a terrible experience. The food was bland and the service was too slow.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['79.51%', '16.45%', '2.21%', '1.11%', '0.72%']}\n",
      "\n",
      "Review: Quite a pleasant surprise! The dishes were creative and flavorful, and the staff was attentive.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.82%', '0.59%', '4.94%', '25.15%', '68.51%']}\n",
      "\n",
      "Review: Not worth the money. The food was mediocre at best and the ambiance wasn't as advertised.\n",
      "Sentiment Analysis Result: {'predicted_class': '2 Stars', 'class_probabilities': ['15.27%', '75.96%', '6.72%', '1.42%', '0.64%']}\n",
      "\n",
      "Review: An excellent choice for our anniversary dinner. We had a delightful evening with great food and service.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['1.09%', '0.53%', '4.60%', '24.22%', '69.56%']}\n",
      "\n",
      "Review: The service was below average and the waiter seemed uninterested.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['53.14%', '39.92%', '4.71%', '1.38%', '0.85%']}\n",
      "\n",
      "Review: Loved the vibrant atmosphere and the innovative menu choices. Will definitely return!\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.84%', '0.71%', '3.43%', '10.00%', '85.02%']}\n",
      "\n",
      "Review: It was just okay, not what I expected from the rave reviews.\n",
      "Sentiment Analysis Result: {'predicted_class': '2 Stars', 'class_probabilities': ['9.37%', '73.47%', '13.24%', '2.99%', '0.93%']}\n",
      "\n",
      "Review: The worst restaurant experience ever. Would not recommend to anyone.\n",
      "Sentiment Analysis Result: {'predicted_class': '1 Star', 'class_probabilities': ['88.76%', '7.32%', '1.71%', '1.21%', '1.01%']}\n",
      "\n",
      "Review: Absolutely fantastic! Can't wait to go back and try more dishes.\n",
      "Sentiment Analysis Result: {'predicted_class': '5 Stars', 'class_probabilities': ['0.76%', '0.70%', '4.39%', '11.77%', '82.37%']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    \"The food at this restaurant was absolutely wonderful, from preparation to presentation, very pleasing.\",\n",
    "    \"I had a terrible experience. The food was bland and the service was too slow.\",\n",
    "    \"Quite a pleasant surprise! The dishes were creative and flavorful, and the staff was attentive.\",\n",
    "    \"Not worth the money. The food was mediocre at best and the ambiance wasn't as advertised.\",\n",
    "    \"An excellent choice for our anniversary dinner. We had a delightful evening with great food and service.\",\n",
    "    \"The service was below average and the waiter seemed uninterested.\",\n",
    "    \"Loved the vibrant atmosphere and the innovative menu choices. Will definitely return!\",\n",
    "    \"It was just okay, not what I expected from the rave reviews.\",\n",
    "    \"The worst restaurant experience ever. Would not recommend to anyone.\",\n",
    "    \"Absolutely fantastic! Can't wait to go back and try more dishes.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for review in reviews:\n",
    "    result = get_sentiment(review)\n",
    "    results.append(result)\n",
    "    print(\"Review:\", review)\n",
    "    print(\"Sentiment Analysis Result:\", result)\n",
    "    print()  # Adding a newline for better readability between results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
